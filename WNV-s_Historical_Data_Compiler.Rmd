---
title: "WNV-FoCo_Sur_PosID"
author: "Toby Koch"
date: "2024-02-12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list = ls())

if (!require("pacman")) install.packages("pacman")
pacman::p_unload()

source("config.R")

#packages used in the analysis
pacman::p_load(pkg_list, character.only = T)
 

```


#READ AND CONSOLIDATE FULLL REPORT
PURPOSE:
Consolidating pcr results from full report

Notes: primarily doing this for 2017 because data was blank but going to see if it 
would work for all years in order to combine all available surveillance data
such as trap and mosquito data.

```{r}
#get list of file paths for all files that contain pattern
t = list.files(pattern = "full\\sreport",
               recursive = T,
               full.names = T,
               ignore.case = T)  

#remove files that contain full report but are not the live data/don't open and are copies
t = t[!grepl("~\\$FC|skeleton|test", t, ignore.case = T)]


convert_to_date <- function(df) {
  # Check if "Trap Date" column exists in the dataframe
  if ("Trap Date" %in% names(df)) { # Check if "Trap Date" column is numeric
    if (is.character(df[["Trap Date"]])) { # Convert "Trap Date" column to date format
      df <- df %>%
        mutate(`Trap Date` = as.Date(as.numeric(`Trap Date`), origin = "1899-12-30")) 
    }
  }
  return(df)
}

#read and combine all the full report files 
data0 = t %>% 
  map(~read_excel(.x, col_names = T,
                  sheet = "Weekly Data Input",
                  col_types = "text")) %>%
  #map(convert_to_date) %>%
  #map(~ .x %>% mutate_if(is.numeric, as.character)) %>%
  bind_rows() %>%
  distinct_all() #getting warnings but data still looks good
```



#READ IN 13 & 14
POSITIVES FROM 2013 AND 2014
need to pull data from consolidated lists for 2013 and 2014 
because Full Report files do not exist for these documents
It seems this was before they had started the full report work
but there are consolidated list files for 2013 and 2014
```{r}


d2013 = read.csv("data_input/2013 WNV consolidation list.csv")

d2013 = d2013 %>%
  filter(!is.na(year)) %>%
  filter(year != "Year")%>%
  mutate(test_code = "1") %>%
  select(any_of(col_keep)) %>%
  mutate_all(trimws)


#2014
d2014 = read_csv("data_input/2014 WNV consolidation list.csv")

d2014 = d2014 %>%
  mutate(test_code = "1") %>%
  rename(trap_type = "method") %>%
  select(any_of(col_keep)) %>%
  mutate_all(trimws)

write.csv(d2014, "data_mid/2014_test.csv")

```



READ IN SEQ SAMPLES TO IDENTIFY POSITIVE SAMPLES THAT AREN'T SEQUENCED
```{r}
seq_samples = read.csv("data_input/mdata_co.csv")
```


#COMBINE FULL REPORT WITH 13 AND 14 AND CLEAN
PURPOSE:
COMBINE 2013 AND 2014 POSITIVES AND CLEAN DATA

NOTES:
rename samples from original
combine with 2013 and 2014
remove - in CSU ID


```{r}
date_col = c("trap_date")
num_col = c("year", "week", "no_gravid","no_deplete", "total", "test_code", "seq")


data_all = clean_names(data0) %>%
  rename(!!!batch_rename) %>%
  select(all_of(col_keep)) %>%
  bind_rows(d2013) %>%
  bind_rows(d2014) %>%
  mutate_all(trimws) %>%
  mutate_all(~str_replace_all(.x, "[^[:print:]]", "")) %>% #remove any weird symbols
  mutate(csu_id = str_replace(csu_id, "-", "")) %>% #remove "-" to match accession 
  mutate(seq = if_else(csu_id %in% seq_samples$accession, 1,0)) %>% #id unseq positives
  mutate(across(all_of(num_col),
                as.numeric)) %>%
    mutate(across(all_of(date_col),
                   ~as.Date(as.numeric(.x), origin = "1899-12-30"))
           ) %>%
  mutate(across(all_of(num_col), replace_na, 0)) %>% #convert the missing gravid/deplete values to 0
  distinct_all()
  
data_na = data_all[!complete.cases(data_all),]

data_all = data_all %>% 
  filter(!is.na(year) & year != "" & year != 0)

data_all %>% group_by(year) %>% count()


#WRITE TO FILE
file_name = paste0("data_output/",
                   min(data_all$year, na.rm = T),"-", 
                   max(data_all$year, na.rm = T),
                   "_WNV_pcr_all_pools.csv")

write_csv(data_all, file_name)
```


Summary statistics of all mosquito pools tested

NOTES:
removes 2013 and 2014 because we only have positives and limited data from then.
```{r}
test_sum = data_all %>% group_by(year) %>%
  filter(year >= 2015) %>% #remove 2014 because we don't have good data from before 2015
  summarise(n = n(),
            pos = sum(test_code)) %>%
  mutate(pct = round(pos/n, 4)*100) %>%
  drop_na()

test_sum_p <- ggplot(test_sum) +
  geom_line(aes(x = year, y = n, color = "n"), size = 1, group = 1) +
  geom_line(aes(x = year, y = pos, color = "pos"), size = 1, group = 1) +
  scale_color_manual(values = c("n" = "blue", "pos" = "red")) +
  labs(title = "WNV Pool Tested Overtime",
       y = "Count",
       color = "Variable") +
  theme_minimal()

# Plot
plot(test_sum_p)

```



Positives consolidation from full report files and consolidation files
```{r}

data_pos = data_all %>%
  filter(test_code == 1)

file_name = paste0("data_output/",
                   min(data_all$year, na.rm = T),"-", 
                   max(data_all$year, na.rm = T),
                   "_WNV_pcr_positives.csv")

write_csv(data_pos, file_name)

```


#GET UNSEQ POS
SCRIPT BELOW PURPOSE: 
Compare list of positives to sequenced samples to find unsequenced positives

NOTES:
2024-02-13: mdata_co.csv is the most available list of FoCo sequences that was provided for 
the nextstrain build. (n = 696)

```{r}

unseq_positives = data_all %>%
  filter(test_code == 1 & seq == 0)

unseq_positives %>% group_by(year) %>%
  count()

# positive_2023 = unseq_positives %>%
#   filter(year == 2023) %>%
#   select(any_of(col_keep))
# 
# write.csv(positive_2023, "data_output/2023_WNV_pcr_postives.csv")
# 
# 
# positive_2022 = unseq_positives %>%
#   filter(year == 2022) %>%
#   select(any_of(col_keep))
# 
# write.csv(positive_2022, "data_output/2022_WNV_pcr_postives.csv")

```




#READ AND BIND PLATEMAPS

```{r}
t = list.files(pattern = "q-RT.*PCR\\sPlate",
               recursive = T,
               full.names = T,
               ignore.case = T) 


t = t[!grepl("~\\$FC|skeleton|test", t, ignore.case = T)]

#t = t[grepl("2021", t)]

#t = sample(t, 3)

read_excel_w_filepath <- function(file) {
  filepath <- file
  data <- read_excel(file,
                    col_names = F, 
                    col_types = "text")
  data$filepath <- basename(file)
  return(data)
}


#read and combine all the full report files 
plate_map0 = t %>% 
  map(read_excel_w_filepath) %>%
  bind_rows() 
```

#CLEAN PLATEMAPS
```{r}


#keep only the actual platemap columns
 plate_map1 = plate_map0[,c(15:29)]

  #filter_at(vars(starts_with("x")), all_vars(!is.na(.))) # gets rid of row if any na for some reason

#plate_map2 = plate_map[53:nrow(plate_map),]
  
#create new column names
new_names = c("file","row", "type",  paste0("col", 1:12))

colnames(plate_map1) = new_names


#plate_map2 = plate_map1[2:nrow(plate_map1),] #remove row that had column names

#remove blank rowss
plate_map2 = plate_map1 %>%
  mutate_all(str_trim) %>%
  #filter(!is.na(type)&type != 'Content') %>%
  filter(type == "Cq" | type == "Sample") %>%
  select(-row)

```


#RESHAPE DATA
```{r}
#add id for cq and sample for pivoting


id = rep(1:ceiling(nrow(plate_map2)/2),2)
id = sort(id)


#separate cq and id to make longer them merge
cq = plate_map2 %>%
  mutate(id = id) %>%
  filter(type == "Cq") %>%
  pivot_longer(cols = starts_with("col"), names_to = "col_name", values_to = "Cq") %>%
  select(-type)

sample = plate_map2 %>%
  mutate(id = id) %>%
  filter(type == "Sample") %>%
  pivot_longer(cols = starts_with("col"), names_to = "col_name", values_to = "sample") %>%
  select(-type)

cq_id = left_join(cq, sample, by = c("file", "col_name", "id")) %>%
  mutate(csu_id = str_remove(sample, "-")) %>%
  mutate(cq = as.numeric(Cq)) %>%
  filter(str_detect(sample, "CSU")) %>%
  select(csu_id, cq, file) %>%
  distinct_all()

#have dupilcates because some samples were run multiple times
dupes = janitor::get_dupes(cq_id, csu_id)

#get list of samples that were run multiple times that had NA or the higher cq value for filtering
rm_cq_id = cq_id %>% distinct_all() %>%
    mutate(dup = duplicated(csu_id) | duplicated(csu_id, fromLast = TRUE)) %>% #find duplicated ids
    filter(dup == TRUE) %>% #keep only duplicated csu_ids
    distinct(csu_id,cq, .keep_all = T) %>% #keep first instance of csu_id and cq are the same
    group_by(csu_id) %>% 
    mutate(high_cq = if_else(cq == max(cq, na.rm = F), T, F)) %>% #find the >>ct for samples that were dupes 
    filter(high_cq == T|is.na(high_cq)) %>% #remove the lower cq values because we want to keep those
    ungroup()

#remove the samples with duplicates taht 
cq_id2 = cq_id %>% 
  anti_join(rm_cq_id, by = c("csu_id", "cq")) %>%
  mutate(cq = if_else(is.na(cq), 55.55, cq)) #make samples with no detectable cq 99 foer identifying unmatched samples in merge

dupes2 = cq_id2 %>% janitor::get_dupes(csu_id)
                   
write.csv(cq_id2, "data_output/cq_values.csv")

```



```{r}
data_all_cq = data_all %>%
  left_join(cq_id2 %>% select(csu_id, cq),
            by = c("csu_id"))

write.csv(data_all_cq, "data_output/2013-2023_WNV_cq_all_data.csv")

unmatched_cq = 
  data_all_cq %>%
  filter(is.na(cq)) 

unmatched_cq_stats = 
  data_all_cq %>%
  filter(is.na(cq)) %>%
  group_by(year,week) %>%
  count()

```


```{r}
# na_cq = data_all_cq %>%
#   filter(cq == 55.55) %>%
#   nrow()
# jit = runif(na_cq, 1,15)

pacman::p_load(wesanderson)
selected_palette <- wes_palette("Darjeeling1")

pos_cq_plot = data_all_cq %>% 
  filter(!is.na(cq)&cq<55) %>%
  #mutate(cq = if_else(cq == 55.55, cq + jit ,cq)) %>% # add vertical jitter to negatives
  ggplot(aes(x = year, y = cq, color = as.factor(test_code), fill = as.factor(test_code))) +
  geom_jitter(aes(text = paste(csu_id,week,cq)), alpha = 0.7,size = 1.5) +
  scale_color_manual(values = selected_palette) +
  scale_fill_manual(values = selected_palette) +
  xlim(2019,2023) +
  labs(color = "Positive",
       fill = "Positive") +
  theme_classic()

pos_cq_plot

ggsave("data_output/pos_cq_plot.png",pos_cq_plot, width = 6, height = 3)

plotly::ggplotly(pos_cq_plot,tooltip = c("text"))

pos_seq_cq_plot = data_all_cq %>% 
  filter(test_code == 1) %>%
ggplot(aes(x = year, y = cq, color = as.factor(seq), fill = as.factor(seq))) +
  geom_jitter() +
  ggtitle("Positive Samples Sequenced by Cq") +
    scale_color_manual(values = selected_palette) +
  scale_fill_manual(values = selected_palette) +
 # xlim(2019,2023) +
  theme_classic()

```

